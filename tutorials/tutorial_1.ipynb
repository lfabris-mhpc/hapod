{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import hapod as hp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_avail_total = hp.get_memory_size()\n",
    "memory_forbidden = 2**30\n",
    "print(f\"total available memory is {memory_avail_total / 2**30:.2f} GB\")\n",
    "print(f\"but {memory_forbidden / 2**30:.2f} GB will be made unavailable\")\n",
    "\n",
    "serializer = hp.NumpySerializer()\n",
    "\n",
    "dtype = np.float64\n",
    "n_rows = 3600000\n",
    "refresh_snapshots = True\n",
    "refresh_chunks = True\n",
    "refresh_hapod = refresh_snapshots or True\n",
    "refresh_rand = refresh_snapshots or True\n",
    "\n",
    "n_svd_max_cols = hp.get_max_svd_columns(n_rows, \n",
    "                                        memory_limit=memory_avail_total - memory_forbidden,\n",
    "                                        dtype=dtype)\n",
    "print(f\"the largest matrix for SVD is {n_rows, n_svd_max_cols} using {dtype}\")\n",
    "\n",
    "n_chunk_max_cols = n_svd_max_cols // 2\n",
    "print(f\"the hapod can use chunks of {n_chunk_max_cols} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = n_chunk_max_cols * 16\n",
    "snapshots_matrix_memory = hp.get_matrix_memory_footprint((n_rows, n_cols), dtype=dtype)\n",
    "print(f\"a snapshots matrix of size {n_rows, n_cols} would use {snapshots_matrix_memory / 2**30:.2f} GB of memory\")\n",
    "print(f\"    SVD would use {hp.get_svd_memory_footprint((n_rows, n_cols), dtype=dtype) / 2**30:.2f} GB of memory\")\n",
    "\n",
    "n_chunks = hp.get_n_chunks_fulltree(n_cols, n_chunk_max_cols=n_chunk_max_cols)\n",
    "print(f\"for a balanced, full, merge tree, will need {n_chunks} chunks with maximum size {n_chunk_max_cols} >= {n_cols / n_chunks:.3f} average\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_randomized_svd_max_cols = hp.get_max_randomized_svd_samples((n_rows, n_cols),\n",
    "                                  memory_limit=memory_avail_total - memory_forbidden,\n",
    "                                  )\n",
    "print(f\"randomized SVD can use {n_randomized_svd_max_cols} samples\")\n",
    "print(f\"which correspond to {hp.get_randomized_svd_memory_footprint((n_rows, n_cols), n_randomized_svd_max_cols, dtype=dtype) / 2**30:.2f} GB of memory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_base import get_test_matrix_full_rank\n",
    "\n",
    "X_fullrank, U_fullrank, s_true = get_test_matrix_full_rank(n_rows= n_cols,\n",
    "                                                            n_cols = n_cols,\n",
    "                                                            dtype = dtype,\n",
    "                                                            return_Us = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = \"/scratch/lfabris/hapod_test\"\n",
    "os.makedirs(work_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots_dir = os.path.join(work_dir, \"snapshots\")\n",
    "print(f\"simulating a snapshot matrix with size {(n_rows, n_cols)} under {snapshots_dir}\")\n",
    "\n",
    "if not os.path.isdir(snapshots_dir) or refresh_snapshots:\n",
    "    print(f\"create snapshots under {snapshots_dir}\")\n",
    "    shutil.rmtree(snapshots_dir)\n",
    "    os.makedirs(snapshots_dir, exist_ok=True)\n",
    "\n",
    "    print(\n",
    "        f\"storing {snapshots_matrix_memory / 2**30:.3f} GB worth of snapshots\"\n",
    "    )\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    elapsed_snapshots = -time.perf_counter()\n",
    "    snapshots_fnames = []\n",
    "    for i in range(n_cols):\n",
    "        x = np.concatenate([X_fullrank[:, i], \n",
    "                                np.zeros(n_rows-n_cols, dtype=dtype)], \n",
    "                            axis=0).reshape(-1, 1)\n",
    "        snapshot_fname = os.path.join(snapshots_dir, f\"snapshot_{i:04d}\")\n",
    "        snapshot_fname = serializer.store(x, snapshot_fname)\n",
    "\n",
    "        snapshots_fnames.append(snapshot_fname)\n",
    "    elapsed_snapshots += time.perf_counter()\n",
    "    print(f\"created {len(snapshots_fnames)} snapshot files in {elapsed_snapshots:.3f}\")\n",
    "else:\n",
    "    snapshots_fnames = list(glob.glob(os.path.join(snapshots_dir, \"*.npy\")))\n",
    "    print(f\"found {len(snapshots_fnames)} snapshot files in {snapshots_dir}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_dir = os.path.join(work_dir, \"chunks\")\n",
    "print(f\"simulating chunks of maximum size {n_rows, n_chunk_max_cols} under {chunks_dir}\")\n",
    "\n",
    "if not os.path.isdir(chunks_dir) or refresh_chunks:\n",
    "    print(f\"create chunks under {chunks_dir}\")\n",
    "    shutil.rmtree(chunks_dir)\n",
    "    os.makedirs(chunks_dir, exist_ok=True)\n",
    "\n",
    "    elapsed_chunks = -time.perf_counter()\n",
    "    chunks_fnames = hp.make_chunks(\n",
    "        snapshots_fnames,\n",
    "        chunks_dir,\n",
    "        n_chunks=n_chunks,\n",
    "        serializer=serializer,\n",
    "    )\n",
    "    elapsed_chunks += time.perf_counter()\n",
    "    print(f\"created {len(chunks_fnames)} column chunks files in {elapsed_chunks:.3f}\")\n",
    "else:\n",
    "    chunks_fnames = list(glob.glob(os.path.join(chunks_dir, \"*.npy\")))\n",
    "    print(f\"found {len(chunks_fnames)} column chunks files in {chunks_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if refresh_hapod:\n",
    "    hapod_tmp_dir = os.path.join(work_dir, \"tmp\")\n",
    "\n",
    "    elapsed_hapod = -time.perf_counter()\n",
    "    U_hapod, s_hapod = hp.hapod(chunks_fnames,\n",
    "                    chunk_rank_max=n_chunk_max_cols,\n",
    "                    temp_work_dir=hapod_tmp_dir,\n",
    "                    serializer=serializer,\n",
    "                    verbose=True)\n",
    "    elapsed_hapod += time.perf_counter()\n",
    "\n",
    "    print(f\"finished hapod in {elapsed_hapod:.3f}\")\n",
    "    print(f\"    U.shape {U_hapod.shape}\")\n",
    "    print(f\"    s_hapod.shape {s_hapod.shape}\")\n",
    "\n",
    "    np.save(os.path.join(work_dir, \"U_hapod.npy\"), U_hapod)\n",
    "    np.save(os.path.join(work_dir, \"s_hapod.npy\"), s_hapod)\n",
    "\n",
    "    U_hapod = None\n",
    "    del U_hapod\n",
    "else:\n",
    "    s_hapod = np.load(os.path.join(work_dir, \"s_hapod.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_randomized_svd_max_cols = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if refresh_rand:\n",
    "    elapsed_rand = -time.perf_counter()\n",
    "    U_rand, s_rand = hp.randomized_pod(chunks_fnames,\n",
    "                                    n_sources_samples=n_randomized_svd_max_cols,\n",
    "                                    serializer=serializer,)\n",
    "    elapsed_rand += time.perf_counter()\n",
    "\n",
    "    print(f\"finished rand in {elapsed_rand:.3f}\")\n",
    "    print(f\"    U_rand.shape {U_rand.shape}\")\n",
    "    print(f\"    s_rand.shape {s_rand.shape}\")\n",
    "\n",
    "    np.save(os.path.join(work_dir, \"U_rand.npy\"), U_rand)\n",
    "    np.save(os.path.join(work_dir, \"s_rand.npy\"), s_rand)\n",
    "\n",
    "    U_rand = None\n",
    "    del U_rand\n",
    "else:\n",
    "    s_rand = np.load(os.path.join(work_dir, \"s_rand.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(work_dir, \"U_hapod.npy\")\n",
    "(_, n_cols_hapod), _ = serializer.peek(fname)\n",
    "ortho_hapod = hp.singular_vectors_orthogonality(np.load(fname)[:n_cols], \n",
    "                                                U_fullrank[:, :n_cols_hapod])\n",
    "\n",
    "fname = os.path.join(work_dir, \"U_rand.npy\")\n",
    "(_, n_cols_rand), _ = serializer.peek(fname)\n",
    "ortho_rand = hp.singular_vectors_orthogonality(np.load(fname)[:n_cols], \n",
    "                                                U_fullrank[:, :n_cols_rand])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(s_true, label=\"true\")\n",
    "plt.semilogy(s_hapod, label=\"hapod\")\n",
    "plt.semilogy(s_rand, label=\"rand\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(-4 , 5 + max(n_randomized_svd_max_cols, n_chunk_max_cols))\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(ortho_hapod, label=\"hapod\")\n",
    "plt.plot(ortho_rand, label=\"rand\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
